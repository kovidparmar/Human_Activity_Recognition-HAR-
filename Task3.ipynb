{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saharsh2k5/ML_ASSIGNMENT_1/blob/main/Task3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akUqB-szn91U",
        "outputId": "318add9f-ccf1-4619-b9c6-e240269f2e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Demonstrate how to use Zero-Shot Learning and Few-Shot Learning to classify human activities based on the featurized accelerometer data. Qualitatively demonstrate the performance of Few-Shot Learning with Zero-Shot Learning. Which method performs better? Why? [1 marks]"
      ],
      "metadata": {
        "id": "7cpQYW3aXcD0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcn2rwc1WWH3",
        "outputId": "fce70d5a-4624-4e15-8e9e-9cba3a0c8855",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.1.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langchain-core<0.3.0,>=0.2.26 (from langchain_groq)\n",
            "  Downloading langchain_core-0.2.35-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq<1,>=0.4.1->langchain_groq)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain_groq) (6.0.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.26->langchain_groq)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3.0,>=0.2.26->langchain_groq)\n",
            "  Downloading langsmith-0.1.106-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain_groq) (24.1)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core<0.3.0,>=0.2.26->langchain_groq)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.26->langchain_groq)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_groq)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_groq) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_groq) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_groq) (2.0.7)\n",
            "Downloading langchain_groq-0.1.9-py3-none-any.whl (14 kB)\n",
            "Downloading groq-0.10.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.35-py3-none-any.whl (394 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.106-py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, h11, jsonpatch, httpcore, httpx, langsmith, groq, langchain-core, langchain_groq\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed groq-0.10.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.2.35 langchain_groq-0.1.9 langsmith-0.1.106 orjson-3.10.7 tenacity-8.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from langchain_groq.chat_models import ChatGroq\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "train_file_path = \"/content/drive/MyDrive/ML Assignment1/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/train/X_train.txt\"\n",
        "features_file = \"/content/drive/MyDrive/ML Assignment1/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/features.txt\"\n",
        "train_file_path_y = \"/content/drive/MyDrive/ML Assignment1/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/train/y_train.txt\"\n",
        "test_file_path = \"/content/drive/MyDrive/ML Assignment1/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/test/X_test.txt\"\n",
        "test_file_path_y = \"/content/drive/MyDrive/ML Assignment1/human+activity+recognition+using+smartphones/UCI HAR Dataset/UCI HAR Dataset/test/y_test.txt\"\n",
        "\n",
        "def data_sorting(test_file_path,num_of_samples,test_file_path_y):\n",
        "  df_x = pd.read_csv(test_file_path, delimiter='\\s+', header=None)\n",
        "\n",
        "  df_standardized_x = df_x.apply(lambda x: (x - x.mean()) / x.std(), axis=1)\n",
        "\n",
        "  with open(features_file, 'r') as file:\n",
        "      feature_names = [line.split()[1] for line in file]\n",
        "\n",
        "  df_standardized_x.columns = feature_names\n",
        "\n",
        "  X_array = df_standardized_x.to_numpy()\n",
        "\n",
        "  pca = PCA(n_components=2)\n",
        "  X_pca = pca.fit_transform(X_array)\n",
        "\n",
        "  df_pca = pd.DataFrame(data=X_pca)\n",
        "\n",
        "  print(\"Original shape:\", X_array.shape)\n",
        "  print(\"Reduced shape:\", df_pca.shape)\n",
        "\n",
        "\n",
        "  df_y = pd.read_csv(test_file_path_y, delimiter='\\s+', header=None)\n",
        "\n",
        "  df_pca_sampled = df_pca.sample(n=num_of_samples, random_state=0)  # You can set random_state for reproducibility\n",
        "\n",
        "  sampled_indices = df_pca_sampled.index\n",
        "\n",
        "  df_y_sampled = df_y.loc[sampled_indices]\n",
        "\n",
        "  X_train = df_pca_sampled\n",
        "  y_train = df_y_sampled\n",
        "\n",
        "  label_mapping = {\n",
        "    1.0: \"WALKING\",\n",
        "    2.0: \"WALKING_UPSTAIRS\",\n",
        "    3.0: \"WALKING_DOWNSTAIRS\",\n",
        "    4.0: \"SITTING\",\n",
        "    5.0: \"STANDING\",\n",
        "    6.0: \"LAYING\"}\n",
        "\n",
        "  y_train = y_train[0].map(lambda x: label_mapping.get(x, \"UNKNOWN\"))\n",
        "\n",
        "  y_train = y_train.astype(str)\n",
        "\n",
        "  return X_train,y_train\n",
        "\n",
        "model_name = \"llama3-70b\"\n",
        "groq_models = {\n",
        "    \"llama3-70b\": \"llama3-70b-8192\",\n",
        "    \"mixtral\": \"mixtral-8x7b-32768\",\n",
        "    \"gemma-7b\": \"gemma-7b-it\",\n",
        "    \"llama3.1-70b\": \"llama-3.1-70b-versatile\",\n",
        "    \"llama3-8b\": \"llama3-8b-8192\",\n",
        "    \"llama3.1-8b\": \"llama-3.1-8b-instant\",\n",
        "    \"gemma-9b\": \"gemma2-9b-it\"\n",
        "}\n",
        "import json\n",
        "\n",
        "with open('/content/drive/MyDrive/key.json') as config_file:\n",
        "    config = json.load(config_file)\n",
        "    Groq_Token = config['groq_token']\n",
        "\n",
        "llm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0)\n"
      ],
      "metadata": {
        "id": "PS2PNPeORhQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgvRKmGMWWH7",
        "outputId": "6201e847-402d-453d-bb27-586790248761",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (7352, 561)\n",
            "Reduced shape: (7352, 2)\n",
            "Zero Shot Testing Accuracy: 0.2\n"
          ]
        }
      ],
      "source": [
        "#ZERO SHOT LEARNING\n",
        "\n",
        "X_test_split,y_test_split=data_sorting(train_file_path,10,train_file_path_y)\n",
        "\n",
        "label_set = [\"LAYING\", \"SITTING\", \"STANDING\", \"WALKING\", \"WALKING_UPSTAIRS\", \"WALKING_DOWNSTAIRS\"]\n",
        "\n",
        "def classify_sample(sample):\n",
        "    features = \", \".join([f\"Feature_{i+1}: {value}\" for i, value in enumerate(sample)])\n",
        "    query = f\"\"\"\n",
        "    * You are a human activities classification model.\n",
        "    * Your task is to analyze the given features and classify them as one of these 6 activities: {', '.join(label_set)}.\n",
        "    * Provide only the activity label.\n",
        "\n",
        "    Features:\n",
        "    {features}\n",
        "    \"\"\"\n",
        "    answer = llm.invoke(query)\n",
        "    return answer.content.strip()\n",
        "\n",
        "\n",
        "def classify_samples(X):\n",
        "    predictions = []\n",
        "    for _, row in X.iterrows():\n",
        "        prediction = classify_sample(row)\n",
        "        predictions.append(prediction)\n",
        "    return pd.Series(predictions)\n",
        "\n",
        "test_predictions = classify_samples(X_test_split)\n",
        "\n",
        "true_test_labels = np.array(y_test_split.astype(str))\n",
        "\n",
        "test_predictions = np.array(test_predictions.astype(str))\n",
        "\n",
        "test_accuracy = accuracy_score(true_test_labels, test_predictions)\n",
        "print(\"Zero Shot Testing Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RhgsX_DWWIB",
        "outputId": "ed6e593a-bcd4-457e-df54-7e1fb9a2471c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (7352, 561)\n",
            "Reduced shape: (7352, 2)\n",
            "Original shape: (2947, 561)\n",
            "Reduced shape: (2947, 2)\n",
            "True Test Labels: ['WALKING' 'LAYING' 'LAYING' 'WALKING_UPSTAIRS' 'WALKING' 'LAYING'\n",
            " 'WALKING_UPSTAIRS' 'LAYING' 'WALKING' 'STANDING']\n",
            "Test Predictions: ['WALKING' 'SITTING' 'LAYING' 'WALKING' 'WALKING' 'LAYING' 'WALKING'\n",
            " 'LAYING' 'WALKING' 'STANDING']\n",
            "Few Shot Testing Accuracy:  0.7\n"
          ]
        }
      ],
      "source": [
        "#FEW SHOT LEARNING\n",
        "X_train,y_train=data_sorting(train_file_path,20,train_file_path_y)\n",
        "X_test_split,y_test_split=data_sorting(test_file_path,10,test_file_path_y)\n",
        "\n",
        "feature_label_dict = {}\n",
        "for i in X_train.index:\n",
        "    features = tuple(X_train.loc[i])\n",
        "    label = y_train.loc[i]\n",
        "    feature_label_dict[features] = label\n",
        "\n",
        "def classify_sample(sample,feature_label_dict):\n",
        "    features = \", \".join([f\"Feature_{i+1}: {value}\" for i, value in enumerate(sample)])\n",
        "    query = f\"\"\"\n",
        "    * You are a human activities classification model.\n",
        "    * Your task is to analyze the given features and classify them as one of these 6 activities: {', '.join(label_set)}.\n",
        "    * Provide only the activity label.\n",
        "\n",
        "    Here are few examples:\n",
        "    {feature_label_dict}\n",
        "\n",
        "    Features:\n",
        "    {features}\n",
        "    \"\"\"\n",
        "    answer = llm.invoke(query)\n",
        "    return answer.content.strip()\n",
        "\n",
        "def classify_samples(X,feature_label_dict):\n",
        "    predictions = []\n",
        "    for _, row in X.iterrows():\n",
        "        prediction = classify_sample(row,feature_label_dict)\n",
        "        predictions.append(prediction)\n",
        "    return pd.Series(predictions)\n",
        "\n",
        "test_predictions = classify_samples(X_test_split,feature_label_dict)\n",
        "\n",
        "true_test_labels = np.array(y_test_split.astype(str))\n",
        "\n",
        "test_predictions = np.array(test_predictions.astype(str))\n",
        "\n",
        "print(\"True Test Labels:\", true_test_labels)\n",
        "print(\"Test Predictions:\", test_predictions)\n",
        "\n",
        "test_accuracy = accuracy_score(true_test_labels, test_predictions)\n",
        "print(\"Few Shot Testing Accuracy: \",test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few-shot learning performs better than zero-shot learning since the features of accelerometer data are extracted well, such as walking, standing, and so on. So, the model of few-shot learning can perform better as there are a sufficient number of examples per class."
      ],
      "metadata": {
        "id": "6ckyyxi0XmIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Quantitatively compare the accuracy of Few-Shot Learning with Decision Trees (You may use a subset of the test set if you encounter rate-limiting issues). Which method performs better? Why? [1 marks]"
      ],
      "metadata": {
        "id": "MneA3YlpXm4K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQpNKIgGWWIE",
        "outputId": "f8e37367-d5d4-4efc-8840-dd1d4f818723",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (7352, 561)\n",
            "Reduced shape: (7352, 2)\n",
            "Original shape: (2947, 561)\n",
            "Reduced shape: (2947, 2)\n",
            "Decision Tree Accuracy on test data:  0.7\n"
          ]
        }
      ],
      "source": [
        "#Decision Tree Classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_train,y_train=data_sorting(train_file_path,30,train_file_path_y)\n",
        "X_test,y_test=data_sorting(test_file_path,10,test_file_path_y)\n",
        "\n",
        "clf = DecisionTreeClassifier(max_depth=4,random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test_split)\n",
        "\n",
        "accuracy = accuracy_score(y_test_split, y_pred)\n",
        "print('Decision Tree Accuracy on test data: ',accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "if the data we are taking is large and well structured then the decision tree classifier performs better whereas if the data is of limited samples then the few shot learning is useful. If we are specifically speaking which is better then the method depends on the depth of the decision tree. A decision tree of optimal depth might be better than few shot learning while a decision tree which is underfitting or overfitting might give lower accuracy than few shot learning. As we can see the results we have taken the minimised random data in which few shot learning performs better than decision trees"
      ],
      "metadata": {
        "id": "TJGadiA-XrGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) What are the limitations of Zero-Shot Learning and Few-Shot Learning in the context of classifying human activities based on featurized accelerometer data? [1 marks]"
      ],
      "metadata": {
        "id": "LarxzFlpXtXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limitations of Zero-shot learning:\n",
        "\n",
        "If the descriptions or attributes for the data are vague or not well-defined, the model might struggle to make accurate predictions for unseen activities.\n",
        "\n",
        "Human activities can vary widely, and if unseen activities are fundamentally different from those seen during training, the model might perform poorly.\n",
        "\n",
        "Human activity data can be highly variable due to differences in individual behavior, context, and sensor placement. ZSL might struggle to generalize across such variability without adequate attribute representation.\n",
        "\n",
        "Limitations of Few-shot learning:\n",
        "\n",
        "Few-Shot Learning aims to classify new activities with very few examples. If the number of examples is too small, the model might not capture enough features to perform accurately.\n",
        "\n",
        "With few examples, there is a risk of overfitting where the model may learn to memorize the few samples rather than generalizing from them.\n",
        "\n",
        "Human activities can have subtle differences and may be very similar to each other. Few-Shot Learning models may find it challenging to differentiate between activities that are highly similar, especially with limited examples."
      ],
      "metadata": {
        "id": "tuw5aPVUXwKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) What does the model classify when given input from an entirely new activity that it hasn't seen before? [0.5 mark]"
      ],
      "metadata": {
        "id": "E6o8ISToXydn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In zero-shot and few-shot learning models, the classification behavior for entirely new activities can differ:\n",
        "\n",
        "Zero-Shot Learning: The model attempts to classify the new activity based on its understanding of the general features and relationships between the known activities. If the new activity shares similarities with the known ones, the model might try to categorize it into the closest matching known class or indicate that it is not part of the known classes.\n",
        "\n",
        "Few-Shot Learning: The model might require a few examples of the new activity to classify it accurately. If it has been given a few examples of the new activity, it can use those examples to understand and classify new instances of the same activity. Without any examples, it might classify it as one of the known classes or indicate that it does not fit well with any of them.\n",
        "\n",
        "In both cases, the model's ability to handle entirely new activities depends on how well it can generalize from the training data or how effectively it can leverage any limited examples provided."
      ],
      "metadata": {
        "id": "_8Ts9f4rXz5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Test the model with random data (ensuring the data has the same dimensions and range as the previous input) and report the results. [0.5 mark]"
      ],
      "metadata": {
        "id": "MpV0QuOJX0ae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "from langchain_groq.chat_models import ChatGroq\n",
        "\n",
        "X_train,y_train=data_sorting(train_file_path,20,train_file_path_y)\n",
        "\n",
        "label_set = [\"LAYING\", \"SITTING\", \"STANDING\", \"WALKING\", \"WALKING_UPSTAIRS\", \"WALKING_DOWNSTAIRS\"]\n",
        "\n",
        "def classify_sample(sample, feature_label_dict):\n",
        "    features = \", \".join([f\"Feature_{i+1}: {value}\" for i, value in enumerate(sample)])\n",
        "    query = f\"\"\"\n",
        "    * You are a human activities classification model.\n",
        "    * Your task is to analyze the given features and classify them as one of these 6 activities: {', '.join(label_set)}.\n",
        "    * Provide only the activity label.\n",
        "\n",
        "    Here are few examples:\n",
        "    {feature_label_dict}\n",
        "\n",
        "    Features:\n",
        "    {features}\n",
        "    \"\"\"\n",
        "    answer = llm.invoke(query)\n",
        "    return answer.content.strip()\n",
        "\n",
        "def classify_samples(X, feature_label_dict):\n",
        "    predictions = []\n",
        "    for _, row in X.iterrows():\n",
        "        prediction = classify_sample(row, feature_label_dict)\n",
        "        predictions.append(prediction)\n",
        "    return pd.Series(predictions)\n",
        "\n",
        "feature_label_dict = {tuple(row): label for idx, row in X_train.iterrows() for label in [y_train.loc[idx]]}\n",
        "\n",
        "def generate_random_data(rows, cols):\n",
        "    return pd.DataFrame(np.random.rand(rows, cols), columns=[f\"Feature_{i+1}\" for i in range(cols)])\n",
        "\n",
        "random_X_train = generate_random_data(X_train.shape[0], X_train.shape[1])\n",
        "\n",
        "random_train_predictions = classify_samples(random_X_train, feature_label_dict)\n",
        "\n",
        "print(\"Random Train Predictions:\", random_train_predictions)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZJfGoSRMX388",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51e68914-7179-489e-bec2-223aec1f6e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (7352, 561)\n",
            "Reduced shape: (7352, 2)\n",
            "Random Train Predictions: 0     STANDING\n",
            "1     STANDING\n",
            "2     STANDING\n",
            "3     STANDING\n",
            "4     STANDING\n",
            "5     STANDING\n",
            "6     STANDING\n",
            "7     STANDING\n",
            "8     STANDING\n",
            "9     STANDING\n",
            "10    STANDING\n",
            "11    STANDING\n",
            "12    STANDING\n",
            "13    STANDING\n",
            "14     WALKING\n",
            "15    STANDING\n",
            "16    STANDING\n",
            "17    STANDING\n",
            "18    STANDING\n",
            "19    STANDING\n",
            "dtype: object\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}